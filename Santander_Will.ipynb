{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santander Value Prediction Challenge\n",
    "\n",
    "#### Eden Trainor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Novelty and Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outlier Detection:** The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.\n",
    "\n",
    "**Novelty Detection:** The training data is not polluted by outliers and we are interested in detecting whether a new observation is an outlier. In this context an outlier is also called a novelty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is Multivariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set test colurs to white for dark theme notebook\n",
    "params = {\"ytick.color\" : \"w\",\n",
    "          \"xtick.color\" : \"w\",\n",
    "          \"axes.labelcolor\" : \"w\",\n",
    "          \"axes.edgecolor\" : \"w\",\n",
    "         }\n",
    "\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_NAME = 'WilliamHoltam' #Configure this with your username\n",
    "DATA_REPO = 'C:/Users/' + USER_NAME + '/Datalytyx/Delivery - Documents/Data Science/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOLDER = 'Santander_Value_Prediction/train.csv'\n",
    "TEST_FOLDER = 'Santander_Value_Prediction/test.csv'\n",
    "\n",
    "df_train = pd.read_csv(DATA_REPO + TRAIN_FOLDER)\n",
    "df_test = pd.read_csv(DATA_REPO + TEST_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4459 entries, 0 to 4458\n",
      "Columns: 4993 entries, ID to 9fc776466\n",
      "dtypes: float64(1845), int64(3147), object(1)\n",
      "memory usage: 169.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49342 entries, 0 to 49341\n",
      "Columns: 4992 entries, ID to 9fc776466\n",
      "dtypes: float64(4991), object(1)\n",
      "memory usage: 1.8+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFInfo:\n",
    "    \n",
    "    def __init__(self, train, test):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        return\n",
    "    \n",
    "    def info(self):\n",
    "        # Nº of rows and colums\n",
    "        print('Train: Rows - '+str(len(self.train)) + ' Columns - ' + str(len(self.train.columns)))\n",
    "        print('Test: Rows - '+str(len(self.test)) + ' Columns - ' + str(len(self.test.columns)))\n",
    "        \n",
    "        # Type of columns\n",
    "        train_col_types = self.train.dtypes\n",
    "        test_col_types = self.train.dtypes\n",
    "        print('-'*60)\n",
    "        print('Train: Type of columns')\n",
    "        print('-'*60)\n",
    "        print(train_col_types.groupby(train_col_types).count())\n",
    "        print('-'*60)\n",
    "        print('Test: Type of columns')\n",
    "        print('-'*60)\n",
    "        print(test_col_types.groupby(test_col_types).count())\n",
    "        \n",
    "        # Missing values?\n",
    "        print('-'*60)\n",
    "        list = []\n",
    "        counts = []\n",
    "        for i in self.train.columns:\n",
    "            list.append(i)\n",
    "            counts.append(sum(self.train[i].isnull()))\n",
    "        print('Train: Nº of columns with missing values')\n",
    "        print('-'*60)\n",
    "        print(sum(counts))\n",
    "        print('-'*60)\n",
    "        list = []\n",
    "        counts = []\n",
    "        for i in self.test.columns:\n",
    "            list.append(i)\n",
    "            counts.append(sum(self.test[i].isnull()))\n",
    "        print('Test: Nº of columns with missing values')\n",
    "        print('-'*60)\n",
    "        print(sum(counts))\n",
    "        \n",
    "        # Zero Rows\n",
    "        print('-'*60)\n",
    "        columns_train_sum = pd.DataFrame(self.train.sum(),columns=['Sum of Row'])\n",
    "        print('Train: Nº of columns with all rows zero: ')\n",
    "        print('-'*60)        \n",
    "        print(str(columns_train_sum[columns_train_sum==0].count()))\n",
    "        print('-'*60)\n",
    "        columns_test_sum = pd.DataFrame(self.test.sum(),columns=['Sum of Row'])\n",
    "        print('Test: Nº of columns with all rows zero: ')\n",
    "        print('-'*60)        \n",
    "        print(str(columns_test_sum[columns_train_sum==0].count()))\n",
    "        print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Rows - 4459 Columns - 4993\n",
      "Test: Rows - 49342 Columns - 4992\n",
      "------------------------------------------------------------\n",
      "Train: Type of columns\n",
      "------------------------------------------------------------\n",
      "int64      3147\n",
      "float64    1845\n",
      "object        1\n",
      "dtype: int64\n",
      "------------------------------------------------------------\n",
      "Test: Type of columns\n",
      "------------------------------------------------------------\n",
      "int64      3147\n",
      "float64    1845\n",
      "object        1\n",
      "dtype: int64\n",
      "------------------------------------------------------------\n",
      "Train: Nº of columns with missing values\n",
      "------------------------------------------------------------\n",
      "0\n",
      "------------------------------------------------------------\n",
      "Test: Nº of columns with missing values\n",
      "------------------------------------------------------------\n",
      "0\n",
      "------------------------------------------------------------\n",
      "Train: Nº of columns with all rows zero: \n",
      "------------------------------------------------------------\n",
      "Sum of Row    256\n",
      "dtype: int64\n",
      "------------------------------------------------------------\n",
      "Test: Nº of columns with all rows zero: \n",
      "------------------------------------------------------------\n",
      "Sum of Row    256\n",
      "dtype: int64\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "initiated_class = DFInfo(train = df_train, test = df_test)\n",
    "initiated_class.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº of IDs on the test dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Is any ID on the test dataset?\n",
    "for i in df_train.ID.values:\n",
    "    c = 0\n",
    "    if i in df_test.ID.values:\n",
    "        c = c + 1\n",
    "print('Nº of ID''s on the test dataset: ' + str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any visible outlier on the target?\n",
    "\n",
    "plt.plot(df_train.ID, df_train.sort_values(by=['target']).target)\n",
    "plt.xlabel('ID')\n",
    "plt.ylabel('Target')\n",
    "plt.title('ID vs Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.distplot(df_train[\"target\"],kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target_log'] = np.log(df_train.target)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.distplot(df_train['target_log'],kde=True)\n",
    "plt.title(\"Train set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target_log'] = np.log(df_train.target)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.distplot(df_train['target_log'],kde=True)\n",
    "plt.title(\"Train set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveCorrelatedFeatures(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    \"\"\"\n",
    "    A class that drops features if the pairwise correlation between features\n",
    "    is greater than the specified corr_threshold. If no corr_threshold is\n",
    "    specified then the corr_threshold is 0.9.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    method : {'pearson', 'kendall', 'spearman'}\n",
    "            * pearson : standard correlation coefficient\n",
    "            * kendall : Kendall Tau correlation coefficient\n",
    "            * spearman : Spearman rank correlation\n",
    "    \n",
    "    test : the test dataset in pandas DataFrame form.\n",
    "        \n",
    "    Authors\n",
    "    -------\n",
    "    William Holtam\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method = 'pearson', corr_threshold = 0.9, print_drop_feat = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Description\n",
    "        -----------\n",
    "        Initialise the transformer object and sets the method, \n",
    "        corr_threshold and print_drop_featas instance variables.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.method = method\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.print_drop_feat = print_drop_feat\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Fit creates a correlation matrix and itterates through it to identify\n",
    "        columns which are correlated to a greater extent than the corr_threshold.\n",
    "        \n",
    "        The column numbers of these columns are appended to the \"drop_cols\" list.\n",
    "        The \"drop_cols\" list is sorted and assigned to the instance variable self.drops.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Creates Correlation Matrix    \n",
    "        corr_matrix = X.corr()\n",
    "        iters = range(len(corr_matrix.columns) - 1)\n",
    "        drop_cols = []\n",
    "        count = 0\n",
    "    \n",
    "        # Iterates through Correlation Matrix Table to find correlated columns\n",
    "        for i in iters:\n",
    "            for j in range(i):\n",
    "                item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "                col = item.columns\n",
    "                row = item.index\n",
    "                val = item.values\n",
    "                \n",
    "                if abs(val) >= self.corr_threshold:\n",
    "                    \n",
    "                    # Prints the correlated feature set and the corr val\n",
    "                    if self.print_drop_feat == True:\n",
    "                        print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                    \n",
    "                    drop_cols.append(i)\n",
    "                    count += 1\n",
    "                    \n",
    "        print(str(count)+\" features have been droped.\")            \n",
    "        self.drops = sorted(set(drop_cols))[::-1]\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Transform indexes the inputed dataframe X for the dropped columns and\n",
    "        drops them from the dataframe.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Drops the correlated columns\n",
    "        for i in self.drops:\n",
    "            col = X.iloc[:, (i+1):(i+2)].columns.values\n",
    "            X = X.drop(col, axis=1)\n",
    "            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = CheckRemoveSimilarCol().fit_transform(df_train.iloc[0:1000,0:1000], 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateColumnRemover(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\"\n",
    "    Class\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        groups = X.columns.to_series().groupby(X.dtypes).groups\n",
    "        dups = []\n",
    "    \n",
    "        for int64, float64 in groups.items():\n",
    "    \n",
    "            columns = X[float64].columns\n",
    "            vs = X[float64]\n",
    "            columns_length = len(columns)\n",
    "            \n",
    "    \n",
    "            for i in range(columns_length):\n",
    "                ia = vs.iloc[:,i].values\n",
    "                for j in range(i+1, columns_length):\n",
    "                    ja = vs.iloc[:,j].values\n",
    "                    if array_equal(ia, ja):\n",
    "                        dups.append(columns[i])\n",
    "                        break\n",
    "                        \n",
    "        self.dups = dups\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = X.drop(self.dups, axis=1)\n",
    "        return X\n",
    "    \n",
    "x = DuplicateColumnRemover().fit_transform(df_train.iloc[0:100,0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dups = duplicate_columns(df_train.iloc[0:100,0:100])\n",
    "# X = df_train.drop(dups, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the log of the target the distribuiton looks more distribuite, more like a normal distribuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of what a correlation plot looks like for only the first few columns. In this plot stronger correlations have brighter colours from blue (negative correlation) to red (positive correlation). The closer a plot element is to grey the closer the (spearman) correlation coefficient is to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_short = df_train.drop(columns=[\"target\"])\n",
    "df_train_short = df_train_short.iloc[:,:]\n",
    "corr = df_train_short.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_train_target.sort_values(0).plot()\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Spearman's Rank\")\n",
    "plt.title(\"Features vs Spearman's Rank\", color=\"w\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop columns will all zero values\n",
    "list_columns_train_drop=[]\n",
    "for i in columns_train_sum[columns_train_sum['Sum of Row']==0].index:\n",
    "    list_columns_train_drop.append(i)\n",
    "df_train = df_train.drop(columns=list_columns_train_drop)\n",
    "len(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify the correlation between the target and the variables\n",
    "corr_train_target_values = []\n",
    "corr_train_target_column = []\n",
    "for i in df_train.columns:\n",
    "    if i in ['ID','target','target_log']:\n",
    "        None\n",
    "    else:\n",
    "        corr = df_train[['target',i]].corr(method='spearman')\n",
    "        corr_train_target_values.append(corr.target[1])\n",
    "        corr_train_target_column.append(i)\n",
    "\n",
    "corr_train_target = pd.DataFrame(corr_train_target_values,index=corr_train_target_column)\n",
    "corr_train_target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_train_target.sort_values(0).plot()\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Spearman's Rank\")\n",
    "plt.title(\"Features vs Spearman's Rank\", color=\"w\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify the correlation between the target and the variables\n",
    "corr_train_target_values = []\n",
    "corr_train_target_column = []\n",
    "for i in df_train.columns:\n",
    "    if i in ['ID','target']:\n",
    "        None\n",
    "    else:\n",
    "        corr = df_train[['target',i]].corr(method='pearson')\n",
    "        corr_train_target_values.append(corr.target[1])\n",
    "        corr_train_target_column.append(i)\n",
    "\n",
    "corr_train_target = pd.DataFrame(corr_train_target_values,index=corr_train_target_column)\n",
    "corr_train_target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_train_target.sort_values(0).plot()\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"PCC\")\n",
    "plt.title(\"Feature vs PCC\", color=\"w\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a low correlation of the variables with the target. Let's see how's the distribuiton off eah variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "X = df_train.drop(columns=['target','target_log','ID'])\n",
    "variable_mean = X.mean()\n",
    "variable_std = X.std()\n",
    "variable_name = X.columns\n",
    "high_indices = np.argsort(variable_mean)[::-1][:50]\n",
    "low_indices = np.argsort(variable_mean)[:50]\n",
    "plt.bar(range(len(variable_mean[high_indices])),variable_mean[high_indices],yerr=variable_std[high_indices])\n",
    "plt.title(\"Variable Mean of High Mean Value Indices with Std Deviation\", color=\"w\")\n",
    "plt.show()\n",
    "plt.bar(range(len(variable_mean[low_indices])),variable_mean[low_indices],yerr=variable_std[low_indices])\n",
    "plt.title(\"Variable Mean of Low Mean Value Indices with Std Deviation\", color=\"w\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a PCA to reduze the amount of variables and standardize data\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = df_train.drop(columns=['target','ID','target_log'])\n",
    "X = preprocessing.scale(X)\n",
    "list_n_comp=[]\n",
    "list_var_ratio=[]\n",
    "n_comp = 100\n",
    "max_list_var_ratio = 0.0\n",
    "while max_list_var_ratio<0.8: #n_comp <= 1000:\n",
    "    print(n_comp)\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca.fit(X)\n",
    "    list_n_comp.append(n_comp)\n",
    "    list_var_ratio.append(sum(pca.explained_variance_ratio_))\n",
    "    max_list_var_ratio = max(list_var_ratio)\n",
    "    print(max_list_var_ratio)\n",
    "    n_comp = n_comp + 100\n",
    "#list_n_comp,list_var_ratio\n",
    "\n",
    "plt.plot(list_n_comp, list_var_ratio)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Variance Ratio')\n",
    "plt.title('PCA')\n",
    "plt.ylim([0,1])\n",
    "plt.axhline(0.8,color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 80% explained ratio is with 900 components\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = df_train.drop(columns=['target','ID','target_log'])\n",
    "X = pd.DataFrame(preprocessing.scale(X),columns = X.columns)\n",
    "pca = PCA(n_components=900)\n",
    "X_pca = pd.DataFrame(pca.fit_transform(X))\n",
    "\n",
    "df_train_pca = df_train[['ID','target','target_log']]\n",
    "df_train_pca[X_pca.columns.values]= X_pca\n",
    "\n",
    "X = df_test.drop(columns=['ID'])\n",
    "X = pd.DataFrame(preprocessing.scale(X),columns = X.columns)\n",
    "pca = PCA(n_components=900)\n",
    "X_pca = pd.DataFrame(pca.fit_transform(X))\n",
    "\n",
    "df_test_pca = pd.DataFrame(df_test['ID'],columns=['ID'])\n",
    "df_test_pca[X_pca.columns.values] = X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pca[X_pca.columns.values].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There a big difference between each variable in terms off the mean and std. Best to standardize the data and reduce the number of variables for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "dim_reduction = PCA()\n",
    "Xc = dim_reduction.fit_transform(scale(features))\n",
    "\n",
    "print('variance explained by the first 2 components: %0.1f%%' % (\n",
    "    sum(dim_reduction.explained_variance_ratio_[:2]*100)\n",
    "))\n",
    "\n",
    "print('variance explained by the last 2 components: %0.1f%%' % (\n",
    "    sum(dim_reduction.explained_variance_ratio_[-2:]*100)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Xc, columns=['comp_'+str(j+1) for j in range(4459)])\n",
    "first_two = df.plot(kind='scatter', x='comp_1', y='comp_2', c='DarkGray', s=50)\n",
    "last_two = df.plot(kind='scatter', x='comp_9', y='comp_10', c='DarkGray', s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = plt.plot(Xc)\n",
    "plt.show(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlying = (Xc[:,-1] < -0.1) | (Xc[:,-2] < -0.1)\n",
    "# print(outlying)\n",
    "print(Xc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "DB = DBSCAN(eps=2.5, min_samples=25)\n",
    "DB.fit(Xc)\n",
    "from collections import Counter\n",
    "print(Counter(DB.labels_),'n')\n",
    "print(df[DB.labels_==-1])\n",
    "Counter({0: 414, -1: 28})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
